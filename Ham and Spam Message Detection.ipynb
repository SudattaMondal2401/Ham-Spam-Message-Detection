{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd1c818",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63513430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4bc02f",
   "metadata": {},
   "source": [
    "# Uploading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe0d7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\Work\\\\My work\\\\Message classification\\\\archive\\\\spam.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c47c265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(path, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f05926",
   "metadata": {},
   "source": [
    "# Preliminary viewing of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8379f7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1037739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7482c171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: v1, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['v1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a1fa31",
   "metadata": {},
   "source": [
    "# Checking for null values and empty reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "708c4dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v1               0\n",
       "v2               0\n",
       "Unnamed: 2    5522\n",
       "Unnamed: 3    5560\n",
       "Unnamed: 4    5566\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4392368",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(\"Unnamed: 2\", inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4985cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(\"Unnamed: 3\", inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96c1e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(\"Unnamed: 4\", inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4a04986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v1    0\n",
       "v2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02456a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "blank = []\n",
    "for i, s, rv in dataset.itertuples():\n",
    "    if rv.isspace()==True:\n",
    "        blank.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2688b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8f58a",
   "metadata": {},
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4cd4c6",
   "metadata": {},
   "source": [
    "### Into matrix of features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "749a340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:, 1]\n",
    "y = dataset.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c796e4",
   "metadata": {},
   "source": [
    "### Applying Spacy preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf74421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11c09a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    doc = nlp(text)\n",
    "    lemma = []\n",
    "    for token in doc:\n",
    "        if token.is_stop==False:\n",
    "            lemma.append(token.lemma_)\n",
    "    processed_text = \" \".join(lemma)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03e50842",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db4509",
   "metadata": {},
   "source": [
    "### Dividing into training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "190a9d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(sss.split(x, y)):\n",
    "    train = train_index\n",
    "    test = test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e047eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = []\n",
    "for i in train:\n",
    "    i1.append(i)\n",
    "    \n",
    "i2 = []\n",
    "for i in test:\n",
    "    i2.append(i)\n",
    "\n",
    "x_train = x.iloc[i1]\n",
    "x_test = x.iloc[i2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb16aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y.iloc[i1]\n",
    "y_test = y.iloc[i2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d54b2a",
   "metadata": {},
   "source": [
    "# Applying transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8bfd412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "y_train_encoded = lb.fit_transform(y_train)\n",
    "y_test_encoded = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4791bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train_encoded = vectorizer.fit_transform(x_train)\n",
    "x_test_encoded = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79396820",
   "metadata": {},
   "source": [
    "# Building the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c0e474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8841efc1",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "782e09e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[949  17]\n",
      " [ 27 122]]\n",
      "The f1 score is 0.8472222222222222\n",
      "The accuracy score is 0.9605381165919282\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       966\n",
      "           1       0.88      0.82      0.85       149\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.93      0.90      0.91      1115\n",
      "weighted avg       0.96      0.96      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier1 = DecisionTreeClassifier()\n",
    "classifier1.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier1.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85e956f",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b4c26ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[965   1]\n",
      " [ 30 119]]\n",
      "The f1 score is 0.8847583643122676\n",
      "The accuracy score is 0.9721973094170404\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       966\n",
      "           1       0.99      0.80      0.88       149\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.90      0.93      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier2 = LogisticRegression()\n",
    "classifier2.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier2.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f36b2",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4dc2fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[965   1]\n",
      " [ 23 126]]\n",
      "The f1 score is 0.9130434782608696\n",
      "The accuracy score is 0.97847533632287\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       966\n",
      "           1       0.99      0.85      0.91       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.92      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier3 = RandomForestClassifier(n_jobs=-1)\n",
    "classifier3.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier3.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1059fe1b",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e801083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[966   0]\n",
      " [ 34 115]]\n",
      "The f1 score is 0.8712121212121211\n",
      "The accuracy score is 0.9695067264573991\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       966\n",
      "           1       1.00      0.77      0.87       149\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.89      0.93      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier4 = MultinomialNB()\n",
    "classifier4.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier4.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f964b00",
   "metadata": {},
   "source": [
    "### K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac48feb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[966   0]\n",
      " [101  48]]\n",
      "The f1 score is 0.4873096446700507\n",
      "The accuracy score is 0.9094170403587444\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95       966\n",
      "           1       1.00      0.32      0.49       149\n",
      "\n",
      "    accuracy                           0.91      1115\n",
      "   macro avg       0.95      0.66      0.72      1115\n",
      "weighted avg       0.92      0.91      0.89      1115\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier5 = KNeighborsClassifier()\n",
    "classifier5.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier5.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0de259",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63596adb",
   "metadata": {},
   "source": [
    "### Linear support vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "092343b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[962   4]\n",
      " [ 11 138]]\n",
      "The f1 score is 0.9484536082474226\n",
      "The accuracy score is 0.9865470852017937\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       966\n",
      "           1       0.97      0.93      0.95       149\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.98      0.96      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "classifier6 = LinearSVC()\n",
    "classifier6.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier6.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b72068",
   "metadata": {},
   "source": [
    "### Sigmoid SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d8601fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[961   5]\n",
      " [ 11 138]]\n",
      "The f1 score is 0.9452054794520547\n",
      "The accuracy score is 0.9856502242152466\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       966\n",
      "           1       0.97      0.93      0.95       149\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.98      0.96      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier7 = SVC(kernel = 'sigmoid')\n",
    "classifier7.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier7.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ab0dc6",
   "metadata": {},
   "source": [
    "### Voting Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf83432a",
   "metadata": {},
   "source": [
    "##### Soft Voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a92393f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[962   4]\n",
      " [ 13 136]]\n",
      "The f1 score is 0.9411764705882352\n",
      "The accuracy score is 0.9847533632286996\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       966\n",
      "           1       0.97      0.91      0.94       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.95      0.97      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "classifier8 = VotingClassifier(\n",
    "    estimators=[('lr', LogisticRegression()), ('svc', SVC(kernel = 'sigmoid', probability = True)), ('rf', RandomForestClassifier())],\n",
    "    voting='soft')\n",
    "classifier8.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier8.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b534e3",
   "metadata": {},
   "source": [
    "##### Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40fa1f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[965   1]\n",
      " [ 19 130]]\n",
      "The f1 score is 0.9285714285714286\n",
      "The accuracy score is 0.9820627802690582\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       966\n",
      "           1       0.99      0.87      0.93       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.94      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "classifier9 = VotingClassifier(\n",
    " estimators=[('lr', LogisticRegression()), ('svc', LinearSVC()), ('rf', RandomForestClassifier())],\n",
    "    voting='hard')\n",
    "classifier9.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier9.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c2892",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c5b9754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[961   5]\n",
      " [  9 140]]\n",
      "The f1 score is 0.9523809523809523\n",
      "The accuracy score is 0.9874439461883409\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       966\n",
      "           1       0.97      0.94      0.95       149\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.98      0.97      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "level0 = list()\n",
    "level0.append(('lr', LogisticRegression()))\n",
    "level0.append(('svm', LinearSVC()))\n",
    "level0.append(('bayes', MultinomialNB()))\n",
    "level0.append(('random', RandomForestClassifier()))\n",
    "\n",
    "level1 = LogisticRegression()\n",
    "\n",
    "classifier10 = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "classifier10.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier10.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
